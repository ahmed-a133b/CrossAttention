{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d6b1a3",
   "metadata": {},
   "source": [
    "# üå± Cross-Attention Plant Disease Classification\n",
    "\n",
    "This notebook implements a Cross-Attention Fusion method for multimodal plant disease classification using images and text descriptions.\n",
    "\n",
    "## üìã Features\n",
    "- **Multimodal Architecture**: Combines image and text features using cross-attention\n",
    "- **Configurable Class Subsets**: Run experiments on specific plant diseases\n",
    "- **Hyperparameter Tuning**: Systematic optimization of model parameters\n",
    "- **Multiple Fusion Strategies**: Compare different multimodal fusion approaches\n",
    "\n",
    "## üöÄ Quick Start\n",
    "1. Run the setup cell to install dependencies\n",
    "2. Upload or download your dataset\n",
    "3. Configure your experiment\n",
    "4. Train the model\n",
    "5. Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e65d22",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch torchvision torchaudio\n",
    "!pip install -q gdown pyyaml matplotlib seaborn plotly\n",
    "!pip install -q scikit-learn opencv-python albumentations\n",
    "!pip install -q dataclasses-json tqdm\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0128cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your project files to Colab\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Upload your project files:\")\n",
    "print(\"Required files:\")\n",
    "print(\"  - cross_attention_model.py\")\n",
    "print(\"  - config_manager.py\")\n",
    "print(\"  - dataset_loader.py\")\n",
    "print(\"  - trainer.py\")\n",
    "print(\"  - colab_dataset_setup.py\")\n",
    "print(\"  - plantwild_prompts.json\")\n",
    "print(\"\\n‚ö†Ô∏è Make sure to upload ALL files before proceeding!\")\n",
    "\n",
    "# Uncomment the next line to upload files interactively\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00778e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Clone from GitHub repository\n",
    "# Replace 'your-username/your-repo' with your actual repository\n",
    "\n",
    "# !git clone https://github.com/your-username/your-repo.git\n",
    "# %cd your-repo\n",
    "\n",
    "# Or download specific files from GitHub raw URLs\n",
    "import urllib.request\n",
    "\n",
    "files_to_download = [\n",
    "    # Replace with your actual raw GitHub URLs\n",
    "    # ('cross_attention_model.py', 'https://raw.githubusercontent.com/user/repo/main/cross_attention_model.py'),\n",
    "    # ('config_manager.py', 'https://raw.githubusercontent.com/user/repo/main/config_manager.py'),\n",
    "    # Add other files...\n",
    "]\n",
    "\n",
    "# for filename, url in files_to_download:\n",
    "#     print(f\"Downloading {filename}...\")\n",
    "#     urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "print(\"‚úÖ Project files ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8654090",
   "metadata": {},
   "source": [
    "## üìä Dataset Setup\n",
    "\n",
    "Choose one of the following methods to load your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43223865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset setup utilities\n",
    "from colab_dataset_setup import (\n",
    "    setup_colab_environment,\n",
    "    load_from_google_drive,\n",
    "    load_from_url,\n",
    "    create_sample_for_testing,\n",
    "    ColabDatasetManager\n",
    ")\n",
    "\n",
    "# Set up the environment\n",
    "dataset_manager = setup_colab_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699412b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load from Google Drive\n",
    "# 1. Upload your dataset to Google Drive\n",
    "# 2. Get the file ID from the sharing link\n",
    "# 3. Replace 'YOUR_FILE_ID' with the actual ID\n",
    "\n",
    "# GOOGLE_DRIVE_FILE_ID = \"YOUR_FILE_ID\"  # Replace with your file ID\n",
    "# dataset_manager = load_from_google_drive(GOOGLE_DRIVE_FILE_ID)\n",
    "\n",
    "print(\"üîó To use Google Drive:\")\n",
    "print(\"1. Upload dataset.zip to Google Drive\")\n",
    "print(\"2. Right-click ‚Üí Get shareable link\")\n",
    "print(\"3. Copy the file ID from the URL\")\n",
    "print(\"4. Replace YOUR_FILE_ID above and uncomment the lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load from direct URL\n",
    "# If you have a direct download link to your dataset\n",
    "\n",
    "# DATASET_URL = \"https://example.com/dataset.zip\"  # Replace with your URL\n",
    "# dataset_manager = load_from_url(DATASET_URL)\n",
    "\n",
    "print(\"üåê To use direct URL:\")\n",
    "print(\"1. Host your dataset.zip on a service with direct download\")\n",
    "print(\"2. Get the direct download URL\")\n",
    "print(\"3. Replace the URL above and uncomment the lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Manual upload (for smaller datasets)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "def manual_dataset_upload():\n",
    "    print(\"üì§ Upload your dataset.zip file:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        if filename.endswith('.zip'):\n",
    "            print(f\"üì¶ Extracting {filename}...\")\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"/content/plant_disease_data/\")\n",
    "            \n",
    "            # Remove the zip file after extraction\n",
    "            os.remove(filename)\n",
    "            print(\"‚úÖ Dataset extracted successfully!\")\n",
    "            break\n",
    "    \n",
    "    # Verify the dataset\n",
    "    dataset_manager.print_dataset_info()\n",
    "    return dataset_manager\n",
    "\n",
    "# Uncomment the next line to upload manually\n",
    "# dataset_manager = manual_dataset_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac22e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Create sample dataset for testing\n",
    "# This creates a small dataset for testing your code\n",
    "\n",
    "dataset_manager = create_sample_for_testing()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è NOTE: This is a sample dataset for testing only!\")\n",
    "print(\"Replace with your actual dataset for real experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset is properly loaded\n",
    "is_valid, info = dataset_manager.verify_dataset()\n",
    "dataset_manager.print_dataset_info()\n",
    "\n",
    "if is_valid:\n",
    "    print(\"\\n‚úÖ Dataset is ready for training!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Dataset setup incomplete. Please check the steps above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834da3ac",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92322a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration modules\n",
    "from config_manager import ModelConfig, ConfigManager, ClassSubsetManager\n",
    "import yaml\n",
    "\n",
    "# Create configuration for your experiment\n",
    "config = ModelConfig(\n",
    "    # Dataset configuration\n",
    "    dataset_path=\"/content/plant_disease_data\",  # Colab path\n",
    "    selected_classes=[],  # Empty = use all classes, or specify subset\n",
    "    \n",
    "    # Model architecture\n",
    "    image_backbone=\"resnet50\",\n",
    "    text_encoder=\"bert-base-uncased\",\n",
    "    feature_dim=768,\n",
    "    fusion_type=\"adaptive\",  # concat, adaptive, bilinear, attention\n",
    "    num_cross_attention_layers=4,\n",
    "    num_attention_heads=12,\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    num_epochs=30,  # Reduce for faster testing\n",
    "    \n",
    "    # Colab-specific paths\n",
    "    output_dir=\"/content/outputs\",\n",
    "    checkpoint_dir=\"/content/checkpoints\",\n",
    "    \n",
    "    # Enable mixed precision for faster training on Colab\n",
    "    mixed_precision=True\n",
    ")\n",
    "\n",
    "# Save configuration\n",
    "config_manager = ConfigManager()\n",
    "config_manager.save_config(config, \"colab_config.yaml\")\n",
    "\n",
    "print(\"‚úÖ Configuration created and saved to colab_config.yaml\")\n",
    "print(f\"üìä Dataset path: {config.dataset_path}\")\n",
    "print(f\"üèóÔ∏è Model: {config.image_backbone} + {config.text_encoder}\")\n",
    "print(f\"üîÑ Fusion: {config.fusion_type}\")\n",
    "print(f\"‚è±Ô∏è Epochs: {config.num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb52375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure class subsets (optional)\n",
    "class_manager = ClassSubsetManager()\n",
    "\n",
    "# Option 1: Use predefined subsets\n",
    "available_subsets = class_manager.get_predefined_subsets()\n",
    "print(\"üìã Available predefined subsets:\")\n",
    "for name, classes in available_subsets.items():\n",
    "    print(f\"  {name}: {len(classes)} classes\")\n",
    "\n",
    "# Option 2: Create custom subset\n",
    "# Example: Apple diseases only\n",
    "apple_diseases = class_manager.create_subset_by_category(['apple'])\n",
    "print(f\"\\nüçé Apple diseases: {len(apple_diseases)} classes\")\n",
    "print(f\"Classes: {apple_diseases[:3]}...\")  # Show first 3\n",
    "\n",
    "# Option 3: Random subset for quick testing\n",
    "quick_test_classes = class_manager.create_random_subset(5)\n",
    "print(f\"\\nüé≤ Quick test subset: {len(quick_test_classes)} classes\")\n",
    "print(f\"Classes: {quick_test_classes}\")\n",
    "\n",
    "# Update config with selected subset (uncomment to use)\n",
    "# config.selected_classes = quick_test_classes\n",
    "# config.num_classes = len(quick_test_classes)\n",
    "# print(f\"\\n‚úÖ Updated config to use {len(quick_test_classes)} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7c418",
   "metadata": {},
   "source": [
    "## üöÄ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with minimal configuration\n",
    "from trainer import Trainer\n",
    "\n",
    "# Create a quick test configuration\n",
    "quick_config = ModelConfig(\n",
    "    dataset_path=\"/content/plant_disease_data\",\n",
    "    selected_classes=quick_test_classes[:3],  # Use only 3 classes\n",
    "    num_classes=3,\n",
    "    batch_size=16,  # Smaller batch for Colab memory\n",
    "    learning_rate=1e-4,\n",
    "    num_epochs=5,   # Just 5 epochs for quick test\n",
    "    eval_frequency=2,\n",
    "    output_dir=\"/content/quick_test_outputs\",\n",
    "    checkpoint_dir=\"/content/quick_test_checkpoints\",\n",
    "    mixed_precision=True\n",
    ")\n",
    "\n",
    "print(\"üß™ Running quick test with 3 classes and 5 epochs...\")\n",
    "print(f\"Classes: {quick_config.selected_classes}\")\n",
    "\n",
    "# Create and run trainer\n",
    "trainer = Trainer(quick_config)\n",
    "test_results = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Quick test completed!\")\n",
    "print(f\"Final test accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"Final test F1-score: {test_results['macro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c64b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training with your configuration\n",
    "from trainer import Trainer\n",
    "\n",
    "print(\"üöÄ Starting full training...\")\n",
    "print(f\"Configuration: {config.num_classes} classes, {config.num_epochs} epochs\")\n",
    "print(f\"Model: {config.image_backbone} + {config.fusion_type} fusion\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(config)\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    test_results = trainer.train()\n",
    "    \n",
    "    print(\"\\nüéâ Training completed successfully!\")\n",
    "    print(\"Final Results:\")\n",
    "    print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {test_results['macro_precision']:.4f}\")\n",
    "    print(f\"  Recall: {test_results['macro_recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {test_results['macro_f1']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    print(\"Try reducing batch_size or num_epochs if running out of memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecfe07f",
   "metadata": {},
   "source": [
    "## üß™ Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different fusion methods\n",
    "fusion_methods = ['concat', 'adaptive', 'bilinear', 'attention']\n",
    "fusion_results = []\n",
    "\n",
    "# Use a small subset for quick comparison\n",
    "test_classes = quick_test_classes[:3]\n",
    "\n",
    "print(\"üîÑ Comparing fusion methods...\")\n",
    "for fusion_type in fusion_methods:\n",
    "    print(f\"\\n--- Testing {fusion_type} fusion ---\")\n",
    "    \n",
    "    # Create configuration for this fusion method\n",
    "    fusion_config = ModelConfig(\n",
    "        dataset_path=\"/content/plant_disease_data\",\n",
    "        selected_classes=test_classes,\n",
    "        num_classes=len(test_classes),\n",
    "        fusion_type=fusion_type,\n",
    "        batch_size=16,\n",
    "        num_epochs=10,\n",
    "        output_dir=f\"/content/fusion_{fusion_type}\",\n",
    "        checkpoint_dir=f\"/content/fusion_{fusion_type}_checkpoints\",\n",
    "        mixed_precision=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        trainer = Trainer(fusion_config)\n",
    "        results = trainer.train()\n",
    "        \n",
    "        fusion_results.append({\n",
    "            'fusion_type': fusion_type,\n",
    "            'accuracy': results['accuracy'],\n",
    "            'f1_score': results['macro_f1']\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ {fusion_type}: Acc={results['accuracy']:.4f}, F1={results['macro_f1']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {fusion_type} failed: {e}\")\n",
    "        fusion_results.append({\n",
    "            'fusion_type': fusion_type,\n",
    "            'accuracy': 0.0,\n",
    "            'f1_score': 0.0\n",
    "        })\n",
    "\n",
    "# Display comparison results\n",
    "print(\"\\nüìä Fusion Method Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "for result in fusion_results:\n",
    "    print(f\"{result['fusion_type']:10s}: Acc={result['accuracy']:.4f}, F1={result['f1_score']:.4f}\")\n",
    "\n",
    "# Find best fusion method\n",
    "best_fusion = max(fusion_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nüèÜ Best fusion method: {best_fusion['fusion_type']} (Acc={best_fusion['accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple hyperparameter search\n",
    "import random\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hp_ranges = {\n",
    "    'learning_rate': [1e-5, 5e-5, 1e-4, 5e-4],\n",
    "    'batch_size': [16, 32],\n",
    "    'feature_dim': [512, 768],\n",
    "    'num_attention_heads': [8, 12]\n",
    "}\n",
    "\n",
    "num_hp_experiments = 3  # Run 3 hyperparameter configurations\n",
    "hp_results = []\n",
    "\n",
    "print(f\"üîç Running {num_hp_experiments} hyperparameter experiments...\")\n",
    "\n",
    "for i in range(num_hp_experiments):\n",
    "    print(f\"\\n--- HP Experiment {i+1}/{num_hp_experiments} ---\")\n",
    "    \n",
    "    # Random hyperparameter selection\n",
    "    hp_config = {\n",
    "        'learning_rate': random.choice(hp_ranges['learning_rate']),\n",
    "        'batch_size': random.choice(hp_ranges['batch_size']),\n",
    "        'feature_dim': random.choice(hp_ranges['feature_dim']),\n",
    "        'num_attention_heads': random.choice(hp_ranges['num_attention_heads'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Hyperparameters: {hp_config}\")\n",
    "    \n",
    "    # Create configuration\n",
    "    exp_config = ModelConfig(\n",
    "        dataset_path=\"/content/plant_disease_data\",\n",
    "        selected_classes=test_classes,\n",
    "        num_classes=len(test_classes),\n",
    "        **hp_config,\n",
    "        num_epochs=8,  # Short epochs for HP search\n",
    "        fusion_type=best_fusion['fusion_type'],  # Use best fusion from previous experiment\n",
    "        output_dir=f\"/content/hp_exp_{i}\",\n",
    "        checkpoint_dir=f\"/content/hp_exp_{i}_checkpoints\",\n",
    "        mixed_precision=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        trainer = Trainer(exp_config)\n",
    "        results = trainer.train()\n",
    "        \n",
    "        hp_results.append({\n",
    "            'config': hp_config,\n",
    "            'accuracy': results['accuracy'],\n",
    "            'f1_score': results['macro_f1']\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Acc={results['accuracy']:.4f}, F1={results['macro_f1']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        hp_results.append({\n",
    "            'config': hp_config,\n",
    "            'accuracy': 0.0,\n",
    "            'f1_score': 0.0\n",
    "        })\n",
    "\n",
    "# Display HP search results\n",
    "print(\"\\nüéØ Hyperparameter Search Results:\")\n",
    "print(\"=\" * 70)\n",
    "for i, result in enumerate(hp_results):\n",
    "    print(f\"Experiment {i+1}:\")\n",
    "    print(f\"  Config: {result['config']}\")\n",
    "    print(f\"  Results: Acc={result['accuracy']:.4f}, F1={result['f1_score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Find best hyperparameters\n",
    "best_hp = max(hp_results, key=lambda x: x['accuracy'])\n",
    "print(f\"üèÜ Best hyperparameters: {best_hp['config']}\")\n",
    "print(f\"   Best accuracy: {best_hp['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633ab18",
   "metadata": {},
   "source": [
    "## üìà Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8132bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize fusion method comparison\n",
    "if fusion_results:\n",
    "    methods = [r['fusion_type'] for r in fusion_results]\n",
    "    accuracies = [r['accuracy'] for r in fusion_results]\n",
    "    f1_scores = [r['f1_score'] for r in fusion_results]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    ax1.bar(methods, accuracies, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Fusion Methods - Accuracy Comparison')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        ax1.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    # F1-score comparison\n",
    "    ax2.bar(methods, f1_scores, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title('Fusion Methods - F1-Score Comparison')\n",
    "    ax2.set_ylabel('F1-Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    for i, v in enumerate(f1_scores):\n",
    "        ax2.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/fusion_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize hyperparameter search results\n",
    "if hp_results:\n",
    "    learning_rates = [r['config']['learning_rate'] for r in hp_results]\n",
    "    accuracies = [r['accuracy'] for r in hp_results]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(learning_rates, accuracies, s=100, alpha=0.7, c='green')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Hyperparameter Search: Learning Rate vs Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, (lr, acc) in enumerate(zip(learning_rates, accuracies)):\n",
    "        plt.annotate(f'{acc:.3f}', (lr, acc), xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.savefig('/content/hyperparameter_search.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"üìä Visualizations saved to /content/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results and model checkpoints\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def create_results_zip():\n",
    "    \"\"\"Create a zip file with all results and models\"\"\"\n",
    "    \n",
    "    zip_filename = \"cross_attention_results.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add configuration files\n",
    "        if os.path.exists(\"colab_config.yaml\"):\n",
    "            zipf.write(\"colab_config.yaml\")\n",
    "        \n",
    "        # Add result visualizations\n",
    "        for img_file in [\"/content/fusion_comparison.png\", \"/content/hyperparameter_search.png\"]:\n",
    "            if os.path.exists(img_file):\n",
    "                zipf.write(img_file, os.path.basename(img_file))\n",
    "        \n",
    "        # Add model checkpoints (if they exist)\n",
    "        checkpoint_dirs = [\"/content/checkpoints\", \"/content/quick_test_checkpoints\"]\n",
    "        for checkpoint_dir in checkpoint_dirs:\n",
    "            if os.path.exists(checkpoint_dir):\n",
    "                for root, dirs, files in os.walk(checkpoint_dir):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arc_path = os.path.relpath(file_path, \"/content\")\n",
    "                        zipf.write(file_path, arc_path)\n",
    "        \n",
    "        # Add output logs\n",
    "        output_dirs = [\"/content/outputs\", \"/content/quick_test_outputs\"]\n",
    "        for output_dir in output_dirs:\n",
    "            if os.path.exists(output_dir):\n",
    "                for root, dirs, files in os.walk(output_dir):\n",
    "                    for file in files:\n",
    "                        if file.endswith(('.json', '.txt', '.csv', '.png')):\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            arc_path = os.path.relpath(file_path, \"/content\")\n",
    "                            zipf.write(file_path, arc_path)\n",
    "    \n",
    "    return zip_filename\n",
    "\n",
    "# Create and download results\n",
    "print(\"üì¶ Creating results package...\")\n",
    "results_zip = create_results_zip()\n",
    "\n",
    "if os.path.exists(results_zip):\n",
    "    print(f\"‚úÖ Results package created: {results_zip}\")\n",
    "    print(f\"üì• Downloading {results_zip}...\")\n",
    "    files.download(results_zip)\n",
    "else:\n",
    "    print(\"‚ùå No results to download\")\n",
    "\n",
    "# Also create a summary report\n",
    "summary_report = \"experiment_summary.txt\"\n",
    "with open(summary_report, 'w') as f:\n",
    "    f.write(\"Cross-Attention Plant Disease Classification - Experiment Summary\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Dataset: {config.dataset_path}\\n\")\n",
    "    f.write(f\"Classes: {config.num_classes}\\n\")\n",
    "    f.write(f\"Selected Classes: {config.selected_classes}\\n\\n\")\n",
    "    \n",
    "    if fusion_results:\n",
    "        f.write(\"Fusion Method Comparison:\\n\")\n",
    "        for result in fusion_results:\n",
    "            f.write(f\"  {result['fusion_type']:10s}: Acc={result['accuracy']:.4f}, F1={result['f1_score']:.4f}\\n\")\n",
    "        f.write(f\"  Best: {best_fusion['fusion_type']} (Acc={best_fusion['accuracy']:.4f})\\n\\n\")\n",
    "    \n",
    "    if hp_results:\n",
    "        f.write(\"Hyperparameter Search Results:\\n\")\n",
    "        for i, result in enumerate(hp_results):\n",
    "            f.write(f\"  Config {i+1}: {result['config']}\\n\")\n",
    "            f.write(f\"    Results: Acc={result['accuracy']:.4f}, F1={result['f1_score']:.4f}\\n\")\n",
    "        f.write(f\"  Best: {best_hp['config']} (Acc={best_hp['accuracy']:.4f})\\n\")\n",
    "\n",
    "files.download(summary_report)\n",
    "print(f\"üìÑ Downloaded summary report: {summary_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b49eae5",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "1. **Scale Up**: Use your full dataset with all classes\n",
    "2. **Extended Training**: Increase epochs for better convergence\n",
    "3. **Advanced Hyperparameter Search**: Use more configurations\n",
    "4. **Model Ensemble**: Combine multiple fusion methods\n",
    "5. **Attention Visualization**: Enable attention map saving\n",
    "6. **Comparison with MVPDR**: Implement baseline comparison\n",
    "\n",
    "## üìù Tips for Google Colab\n",
    "\n",
    "- **GPU Runtime**: Make sure you're using GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- **Memory Management**: Reduce batch_size if you get OOM errors\n",
    "- **Session Limits**: Save checkpoints frequently; Colab sessions timeout after 12 hours\n",
    "- **Data Persistence**: Mount Google Drive to save results permanently\n",
    "- **Mixed Precision**: Always use mixed_precision=True for faster training\n",
    "\n",
    "## üîó Useful Commands\n",
    "\n",
    "```python\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Monitor GPU memory\n",
    "!nvidia-smi\n",
    "\n",
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
